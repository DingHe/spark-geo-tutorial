% Spark tutorial 
% Timothy Hunter
% July 2012

Spark geo tutorial
===================

This is a small tutorial intended to show how to use [spark](http://spark-project.org) 
for some geocoded vizualization. At this end 
of this tutorial, you will be able to infer the common trip patterns of [taxicabs in San Francisco].

In this tutorial, we will load a dataset with spark, make some preliminary analysis and plot some features of the
data in the web browser. Then we will run a simple clustering algorithm that gives more insight on the data.

[taxicabs in San Francisco]: http://www.youtube.com/watch?v=OxCPL4KsDfI&feature=plcp

Getting started
----------------

We will run spark on [EC2] using data already stored in [S3]

[EC2]: http://aws.amazon.com/ec2
[S3]: http://aws.amazon.com/s3


*Setting up the spark and creating an ec2 account*

Launch a small cluster:

```bash
cd spark/ec2
./spark-ec2 -k radlab_mm_tjhunter -i ~/.ssh/radlab_mm_tjhunter.pem -t m1.large -m m1.large -s 10 launch geo-tutorial
```

Wait for it to start...

Make sure we are running with the latest version of spark. In the remote master:

```bash
cd spark/
git pull origin
./sbt/sbt update compile publish-local
~/mesos-ec2/copy-dir ~/spark/
```

Let us use a spark shell for now:

```bash
# TODO: run SparkPi instead
MASTER=localhost:5050 ./spark-shell
```

This should run fine. Now go out by hitting `Ctrl+D`, we are going to download the files for this tutorial.

```bash
cd ~
git clone git://github.com/tjhunter/spark-geo-tutorial.git
cd spark-geo-tutorial
./sbt/sbt update compile publish-local
```

We implemented a host of useful code to save you from some typing during this tutorial.
These commands will make sure this code is available to spark during our interactive session.
This also shows you how to use your favorite java library with spark

We are going to send the jar files (cached in ivy) to the slaves, and the 
classes generated by the spark-geo-tutorial project.

```bash
~/mesos-ec2/copy-dir ~/.ivy2/
~/mesos-ec2/copy-dir ~/spark-geo-tutorial/target/
export SPARK_CLASSPATH=`./sbt/sbt get-jars | grep .ivy2`
```

Running spark
--------------

In order to start spark, you also need to provide some code dependencies that we will be using to manipulate time objects. 
Also, you will not need to type some useful code to manipulate the data structures.


```bash
cd ~/spark
MASTER=localhost:5050 ./spark-shell
```

Now that you are in the spark shell, let s get the data in. First we will import some useful utilities:

```scala
import spark.tutorial.geo.GeoTutorialUtils._
import spark.tutorial.geo._
import spark._
```

The taxi dataset is stored in a file (replace with hadoop):

In the cluster:

```scala
val fname = "s3n://cabspotting-data/20*.txt"
val numSplits = 40
val raw_data = sc.textFile(fname)
```

Nothing has happened yet. Now spark will load the file and return the size of the dataset:

```scala
println("Number of raw data points: " + raw_data.count)
```

This can take a while depending on the number of machines and the network bandwidth. Have a look at the data:
  
```scala
raw_data.first
```

> String = aslagni 0 37.7656517029 -122.407623291 2011-01-14T00:00:00

Each line is an observation for a taxi. It has an ID for the driver, a status (is the taxi hired or not hired),
the latitude and longitude and a timestamp. We will represent it in the code with the following class:

```scala
  case class Observation(
    val id: String,
    val hired: Boolean,
    val date: DateTime,
    val location: Coordinate)

  case class Coordinate(val lat: Double, val lon: Double)
```

To save you some typing, we have implemented a function (`spark.tutorial.geo.GeoTutorialUtils.stringToObservation`)
that reads text lines into an `Observation` object. You can map the raw data (a bunch of strings) into a set of 
`Observation` objects:

```scala
  val observations = raw_data.map(s => stringToObservation(s))
```

You can now manipulate the observations as a regular collections. For example, the first observations we have is:

```scala
observations.first
```

  > res2: spark.tutorial.geo.Observation = Observation(aslagni,false,2011-01-14T00:00:00.000-08:00,Coordinate(37.7656517029,-122.407623291))

Just to see them on a map, we are going to sample a few observations ans display them, using the `sample` method:

```scala
val sampleObservations = observations.sample(false, 1e-5, numSplits)
```

How many did we get?

```scala
sampleObservations.count
```

This is enough to get a rough estimate. We are now going to vizualize this data in the browser. For your convenience,
we have added a few methods that converts all the geo objects into the WKT (well known text) format. You can
plot the data in your browser using the web page [here](http://www.eecs.berkeley.edu/~tjhunter/sparkdemo/geojson.html)

```scala
val localSampleObservations = sampleObservations.collect
println(locationsToWKTString(localSampleObservations.map(_.location)))
```

You can copy the resulting string in the browser. You should get a display like [this](http://www.eecs.berkeley.edu/~tjhunter/sparkdemo/geojson.html?external=examplepoints.wkt):

Now we are going to extract the taxi trips from this data. A taxi trip is a sequence of hired points, followed and 
starting with non-hired points. First, we will partition all the observations by day and by driver, and then work
on each of the subsequences:

```scala
  val by_date_drivers = observations.groupBy(datum => (datum.date.toYearMonthDay(), datum.id))
```

We have already implemented a function to extract the sequences of points for you in `spark.tutorial.geo.GeoTutorialUtils.splitIntoTaxiTrips`)
Now we can get all the taxi trips:

```scala
  val taxiTrips = by_date_drivers.flatMap({ case (key, seq) => splitIntoTaxiTrips(seq) })
```

Since we are going to make some repeated calls to this dataset, we will ask spark to cache it in memory:

```scala
  val cachedTaxiTrips = taxiTrips.cache()
```

Now, we are going to cluster the trips by origin and destination, using the K-means algorithm.

**Explain Kmeans and the code**

Here is the complete code for the clustering step

```scala
val numClusters = 100
val numIterations = 10
var clusterCenters = (for (idx <- 0 until numClusters) yield {
  val datum = localSampleObservations(idx % localSampleObservations.size)
  ((datum.location, datum.location), 0)
}).toArray

var clustered_trips: RDD[(Int, TaxiTrip)] = null
var clustered_trips_centers: Array[((Coordinate, Coordinate), Int)] = null

for (iter <- 0 until numIterations) {
  val clusterCenters_bc = sc.broadcast(clusterCenters)
  clustered_trips_centers = clusterCenters
  // Aggregate the taxi trips by closeness to each centroid
  clustered_trips = cachedTaxiTrips.map(trip => {
    val distances = clusterCenters_bc.value.map(center => {
      val ((from, to), _) = center
      val d1 = trip.start.distanceTo(from)
      val d2 = trip.end.distanceTo(to)
      math.sqrt(d1 * d1 + d2 * d2)
    })
    // Find the argmin
    val minDistance = distances.min
    val minIdx = distances.findIndexOf(_ == minDistance)
    (minIdx, trip)
  })
  // Recompute the center of each centroid
  val tripsByCentroid = clustered_trips.groupBy(_._1).mapValues(_.map(_._2))
  val newCenters = tripsByCentroid.mapValues(trips => {
    var count = 0
    var start_lat = 0.0
    var start_lon = 0.0
    var end_lat = 0.0
    var end_lon = 0.0
    for (trip <- trips) {
      count += 1
      start_lat += trip.start.lat
      start_lon += trip.start.lon
      end_lat += trip.end.lat
      end_lon += trip.end.lon
    }
    ((new Coordinate(start_lat / count, start_lon / count), new Coordinate(end_lat / count, end_lon / count)), count)
  })
  val centers = newCenters.collect().map(_._2).sortBy(-_._2)
  println("New centers:\n")
  for (((start, from), size) <- centers) {
    println("%d :  %s".format(size, TaxiTrip("", start, from).toWKT))
  }
  clusterCenters = centers
  println(wkt2(clusterCenters.take(50).map(z => new TaxiTrip("", z._1._1, z._1._2))))
}
```

Plot all the centroids. Each of them is a pair of (trip start coordinate, trip end coordinate) that
corresponds to some "mean taxi trip" for this cluster.

```scala
println(wkt2(clusterCenters.take(50).map(z => new TaxiTrip("", z._1._1, z._1._2))))
```

Talk about the airport and the south bay.

Each cluster is a set of taxi trip. We are going to look at the start locations and end locations 
for the most important of them.

The largest clusters have tens of thousands of trips, so we cannot plot them easily. We are going to 
plot the bounding box of the start locations and the bounding box for the end location instead.
The bounding box is a perfect candidate for a reduce job:

```scala
val cached_clustered_trips = clustered_trips.cache()

def union(b: BoundingBox, c: Coordinate) = b.union(c)
def combo(b1: BoundingBox, b2: BoundingBox) = b1.union(b2)
for (cluster_id <- Array(10)) {
  val cluster_trips = cached_clustered_trips.filter(_._1 == cluster_id).map(_._2)
  val fromBBox = cluster_trips.map(_.start).aggregate(emptyBoundingBox)(union _, combo _)
  val toBBox = cluster_trips.map(_.end).aggregate(emptyBoundingBox)(union _, combo _)
  println("Detailled analysis of cluster #" + cluster_id)
  val (start, from) = clustered_trips_centers(cluster_id)._1
  println("GEOMETRYCOLLECTION(%s,\n%s,\n%s)" format (fromBBox.toWKT, TaxiTrip("", start, from).toWKT, toBBox.toWKT))
}
```


Explain [final](http://www.eecs.berkeley.edu/~tjhunter/sparkdemo/geojson.html?external=cluster0)
[outputs](http://www.eecs.berkeley.edu/~tjhunter/sparkdemo/geojson.html?external=cluster1) of 
[clusters](http://www.eecs.berkeley.edu/~tjhunter/sparkdemo/geojson.html?external=cluster10)

Cleanup
-------

Do not forget to kill your amazon instances at the end!

```bash
cd spark/ec2
./spark-ec2 -k radlab_mm_tjhunter -i ~/.ssh/radlab_mm_tjhunter.pem destroy geo-tutorial
```
